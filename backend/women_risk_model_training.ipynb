{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebea74ce",
   "metadata": {},
   "source": [
    "# Women Harassment Risk Predictor - ML Model Training\n",
    "\n",
    "This notebook contains the complete pipeline for training a machine learning model to predict harassment risk levels based on various factors.\n",
    "\n",
    "## Pipeline Steps:\n",
    "1. Import Required Libraries\n",
    "2. Load and Explore Dataset\n",
    "3. Data Cleaning and Preprocessing\n",
    "4. Feature Engineering\n",
    "5. Model Training and Comparison\n",
    "6. Model Evaluation\n",
    "7. Hyperparameter Tuning\n",
    "8. Save Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca302cc8",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5308f306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "# Machine Learning models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Model evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99578d56",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51083b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = \"data/women_risk.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET LOADED SUCCESSFULLY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"Number of Rows: {df.shape[0]}\")\n",
    "print(f\"Number of Columns: {df.shape[1]}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171ada8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e3a8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(df.describe())\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Column names\n",
    "print(\"\\nColumn Names:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488d87df",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687c78ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "print(missing[missing > 0] if missing.sum() > 0 else \"No missing values found!\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nDuplicate rows: {duplicates}\")\n",
    "\n",
    "# Remove Timestamp column if it exists\n",
    "if 'Timestamp' in df.columns:\n",
    "    df = df.drop('Timestamp', axis=1)\n",
    "    print(\"\\nâœ… Timestamp column removed\")\n",
    "\n",
    "# Drop any rows with missing values\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    df = df.dropna()\n",
    "    print(f\"âœ… Rows with missing values removed\")\n",
    "\n",
    "# Remove duplicates\n",
    "if duplicates > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"âœ… Duplicate rows removed\")\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d65910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables using Label Encoding\n",
    "print(\"=\"*60)\n",
    "print(\"ENCODING CATEGORICAL VARIABLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"\\nCategorical columns found: {len(categorical_cols)}\")\n",
    "\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "    print(f\"âœ… Encoded: {col}\")\n",
    "\n",
    "# Save label encoders\n",
    "os.makedirs('models', exist_ok=True)\n",
    "joblib.dump(label_encoders, 'models/label_encoders.pkl')\n",
    "print(\"\\nâœ… Label encoders saved to 'models/label_encoders.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e30941",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering and Target Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72214612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary target variable based on risk level (last column)\n",
    "# Assuming the last column contains risk level information\n",
    "target_col = df.columns[-1]\n",
    "print(f\"Target column: {target_col}\")\n",
    "print(f\"\\nUnique values in target: {df[target_col].unique()}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df[target_col].value_counts())\n",
    "\n",
    "# Create binary risk variable: 1 for high/moderate risk, 0 for low/no risk\n",
    "# Adjust threshold based on your data (e.g., >= 2 means high risk)\n",
    "df['risk'] = (df[target_col] >= df[target_col].median()).astype(int)\n",
    "\n",
    "print(f\"\\nBinary risk distribution:\")\n",
    "print(df['risk'].value_counts())\n",
    "print(f\"\\nRisk percentage:\")\n",
    "print(df['risk'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6a6107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "plt.figure(figsize=(14, 10))\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1)\n",
    "plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation with target variable\n",
    "print(\"\\nCorrelation with risk (target):\")\n",
    "target_corr = correlation_matrix['risk'].sort_values(ascending=False)\n",
    "print(target_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f54a576",
   "metadata": {},
   "source": [
    "## 5. Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2238aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(['risk', target_col], axis=1)\n",
    "y = df['risk']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Split data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples ({80}%)\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples ({20}%)\")\n",
    "\n",
    "print(f\"\\nTraining set target distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTesting set target distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0089557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for clarity\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, 'models/scaler.pkl')\n",
    "print(\"âœ… Feature scaling completed\")\n",
    "print(\"âœ… Scaler saved to 'models/scaler.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0742f2",
   "metadata": {},
   "source": [
    "## 6. Train Multiple Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bea0c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"SVM\": SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING MULTIPLE MODELS WITH 5-FOLD CROSS-VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Model':<25} {'Mean Accuracy':<15} {'Std Dev':<10}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Perform 5-fold cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'mean_accuracy': cv_scores.mean(),\n",
    "        'std_dev': cv_scores.std(),\n",
    "        'cv_scores': cv_scores\n",
    "    }\n",
    "    \n",
    "    print(f\"{name:<25} {cv_scores.mean():<15.4f} {cv_scores.std():<10.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae447d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "model_names = list(results.keys())\n",
    "mean_accuracies = [results[name]['mean_accuracy'] for name in model_names]\n",
    "std_devs = [results[name]['std_dev'] for name in model_names]\n",
    "\n",
    "plt.bar(model_names, mean_accuracies, yerr=std_devs, capsize=5, \n",
    "        color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "plt.xlabel('Model', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "plt.title('Model Comparison (5-Fold Cross-Validation)', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim([min(mean_accuracies) - 0.05, 1.0])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(results, key=lambda x: results[x]['mean_accuracy'])\n",
    "print(f\"\\nðŸ† Best Model: {best_model_name}\")\n",
    "print(f\"   Mean Accuracy: {results[best_model_name]['mean_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cac84b5",
   "metadata": {},
   "source": [
    "## 7. Train and Evaluate Best Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052fe6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train best model on full training set\n",
    "best_model = results[best_model_name]['model']\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL PERFORMANCE ON TEST SET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel: {best_model_name}\")\n",
    "print(f\"\\nAccuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6050a7f",
   "metadata": {},
   "source": [
    "## 8. Visualize Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc0fd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Low Risk', 'High Risk'],\n",
    "            yticklabels=['Low Risk', 'High Risk'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bbe55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title(f'ROC Curve - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558bf4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred, target_names=['Low Risk', 'High Risk']))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662d7c1d",
   "metadata": {},
   "source": [
    "## 9. Make Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on a few test samples\n",
    "n_samples = 5\n",
    "sample_indices = np.random.choice(X_test.index, n_samples, replace=False)\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    sample = X_test_scaled.loc[idx:idx]\n",
    "    prediction = best_model.predict(sample)[0]\n",
    "    probability = best_model.predict_proba(sample)[0]\n",
    "    actual = y_test.loc[idx]\n",
    "    \n",
    "    risk_label = \"HIGH RISK\" if prediction == 1 else \"LOW RISK\"\n",
    "    actual_label = \"HIGH RISK\" if actual == 1 else \"LOW RISK\"\n",
    "    \n",
    "    print(f\"\\nSample {idx}:\")\n",
    "    print(f\"  Predicted: {risk_label} (Probability: {probability[1]:.2%})\")\n",
    "    print(f\"  Actual:    {actual_label}\")\n",
    "    print(f\"  Correct:   {'âœ… Yes' if prediction == actual else 'âŒ No'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac970fae",
   "metadata": {},
   "source": [
    "## 10. Save the Trained Model\n",
    "\n",
    "Save the best performing model for deployment in the Flask API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d5c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_path = 'models/women_risk_model.pkl'\n",
    "joblib.dump(best_model, model_path)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL SAVED SUCCESSFULLY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nâœ… Model: {best_model_name}\")\n",
    "print(f\"âœ… Saved to: {model_path}\")\n",
    "print(f\"âœ… Scaler saved to: models/scaler.pkl\")\n",
    "print(f\"âœ… Label encoders saved to: models/label_encoders.pkl\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Create model info file\n",
    "model_info = f\"\"\"\n",
    "Women Harassment Risk Predictor - Model Information\n",
    "====================================================\n",
    "\n",
    "Model Type: {best_model_name}\n",
    "Training Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "Dataset Information:\n",
    "- Total Samples: {len(df)}\n",
    "- Training Samples: {len(X_train)}\n",
    "- Testing Samples: {len(X_test)}\n",
    "- Number of Features: {X_train.shape[1]}\n",
    "\n",
    "Model Performance (Test Set):\n",
    "- Accuracy:  {accuracy:.4f}\n",
    "- Precision: {precision:.4f}\n",
    "- Recall:    {recall:.4f}\n",
    "- F1-Score:  {f1:.4f}\n",
    "- ROC-AUC:   {roc_auc:.4f}\n",
    "\n",
    "Files Generated:\n",
    "- models/women_risk_model.pkl (trained model)\n",
    "- models/scaler.pkl (feature scaler)\n",
    "- models/label_encoders.pkl (categorical encoders)\n",
    "\"\"\"\n",
    "\n",
    "with open('models/model_info.txt', 'w') as f:\n",
    "    f.write(model_info)\n",
    "\n",
    "print(\"âœ… Model information saved to: models/model_info.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc02fb25",
   "metadata": {},
   "source": [
    "## âœ… Training Complete!\n",
    "\n",
    "The model has been trained successfully and saved. You can now:\n",
    "\n",
    "1. **Start the Flask API server:**\n",
    "   ```bash\n",
    "   python app.py\n",
    "   ```\n",
    "\n",
    "2. **Access the dashboard:**\n",
    "   - Open browser: http://127.0.0.1:5000/dashboard\n",
    "   \n",
    "3. **Test predictions:**\n",
    "   - Fill in the questionnaire form\n",
    "   - Get real-time risk predictions\n",
    "   \n",
    "4. **View statistics:**\n",
    "   - Dashboard shows total responses\n",
    "   - Risk level distribution\n",
    "   - High risk cases count\n",
    "\n",
    "### Next Steps:\n",
    "- Review model performance metrics above\n",
    "- Test the API with different inputs\n",
    "- Monitor prediction accuracy\n",
    "- Retrain model with more data as needed"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
