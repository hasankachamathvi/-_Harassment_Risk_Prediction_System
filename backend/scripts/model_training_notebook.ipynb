{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95244478",
   "metadata": {},
   "source": [
    "# Women Harassment Risk Predictor - Model Training\n",
    "\n",
    "This notebook contains the complete machine learning pipeline for training a harassment risk prediction model.\n",
    "\n",
    "## Overview\n",
    "This notebook covers:\n",
    "1. **Import Required Libraries** - Load all necessary ML and visualization libraries\n",
    "2. **Load and Prepare Data** - Load processed dataset and split into train/test sets\n",
    "3. **Train Multiple Models** - Train and compare different ML algorithms\n",
    "4. **Evaluate Model Performance** - Assess model performance with various metrics\n",
    "5. **Hyperparameter Tuning** - Optimize the best performing model\n",
    "6. **Save Trained Model** - Save the final model for deployment\n",
    "7. **Visualize Training History** - View model comparison and performance visualizations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5241cbdb",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for data processing, machine learning, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4568882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning models\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Model evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    f1_score, roc_auc_score, confusion_matrix, \n",
    "    classification_report, roc_curve\n",
    ")\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007c7b29",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data\n",
    "\n",
    "Load the processed dataset from CSV file and prepare it for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c51fa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed dataset\n",
    "data_path = \"../data/women_risk_processed.csv\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING PROCESSED DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"\\nâœ… Dataset loaded successfully!\")\n",
    "print(f\"Shape: {data.shape}\")\n",
    "print(f\"Number of Rows: {data.shape[0]}\")\n",
    "print(f\"Number of Columns: {data.shape[1]}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f5cd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features (X) and target (y)\n",
    "target_col = 'risk'\n",
    "X = data.drop(target_col, axis=1)\n",
    "y = data[target_col]\n",
    "\n",
    "# Split into training and testing sets\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA SPLIT INTO TRAIN AND TEST SETS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]} ({(1-test_size)*100:.0f}%)\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} ({test_size*100:.0f}%)\")\n",
    "print(f\"\\nFeatures: {X_train.shape[1]}\")\n",
    "\n",
    "print(f\"\\nTraining set target distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTesting set target distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf647e3",
   "metadata": {},
   "source": [
    "## 3. Train Multiple Models\n",
    "\n",
    "Train and compare multiple classification algorithms using 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6845ca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple classification models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"SVM\": SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING MULTIPLE MODELS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nPerforming 5-fold cross-validation for each model...\\n\")\n",
    "print(f\"{'Model':<25} {'Mean Accuracy':<15} {'Std Dev':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Perform cross-validation\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'mean_accuracy': scores.mean(),\n",
    "        'std_dev': scores.std(),\n",
    "        'scores': scores\n",
    "    }\n",
    "    print(f\"{name:<25} {scores.mean():<15.4f} {scores.std():<10.4f}\")\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(results, key=lambda x: results[x]['mean_accuracy'])\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"ðŸ† BEST MODEL: {best_model_name}\")\n",
    "print(f\"   Mean Accuracy: {results[best_model_name]['mean_accuracy']:.4f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b989c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "model_names = list(results.keys())\n",
    "mean_accuracies = [results[name]['mean_accuracy'] for name in model_names]\n",
    "std_devs = [results[name]['std_dev'] for name in model_names]\n",
    "\n",
    "plt.bar(model_names, mean_accuracies, yerr=std_devs, capsize=5, \n",
    "        color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "plt.xlabel('Model', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "plt.title('Model Comparison (5-Fold Cross-Validation)', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim([min(mean_accuracies) - 0.05, 1.0])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276b7682",
   "metadata": {},
   "source": [
    "## 4. Evaluate Model Performance\n",
    "\n",
    "Train the best model on full training set and evaluate its performance on the test set using various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3535d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best model on full training set\n",
    "best_model = results[best_model_name]['model']\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='binary')\n",
    "recall = recall_score(y_test, y_pred, average='binary')\n",
    "f1 = f1_score(y_test, y_pred, average='binary')\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"MODEL PERFORMANCE - {best_model_name}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Metric':<20} {'Score':<10}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"{'Accuracy:':<20} {accuracy:<10.4f}\")\n",
    "print(f\"{'Precision:':<20} {precision:<10.4f}\")\n",
    "print(f\"{'Recall:':<20} {recall:<10.4f}\")\n",
    "print(f\"{'F1-Score:':<20} {f1:<10.4f}\")\n",
    "print(f\"{'ROC-AUC:':<20} {roc_auc:<10.4f}\")\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd23270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification report\n",
    "print(\"=\" * 60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_pred, target_names=['Low Risk (0)', 'High Risk (1)']))\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7a0c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "            xticklabels=['Low Risk (0)', 'High Risk (1)'],\n",
    "            yticklabels=['Low Risk (0)', 'High Risk (1)'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConfusion Matrix Values:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228d8247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title(f'ROC Curve - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc6e6e5",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning\n",
    "\n",
    "Optimize the best model's hyperparameters using GridSearchCV to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3348801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grids for different models\n",
    "param_grids = {\n",
    "    \"Random Forest\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [None, 5, 10, 15],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4]\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "        \"max_depth\": [3, 5, 7],\n",
    "        \"min_samples_split\": [2, 5, 10]\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"C\": [0.01, 0.1, 1, 10, 100],\n",
    "        \"penalty\": ['l2'],\n",
    "        \"solver\": ['lbfgs', 'liblinear']\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"max_depth\": [None, 5, 10, 15, 20],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "        \"criterion\": ['gini', 'entropy']\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"C\": [0.1, 1, 10],\n",
    "        \"kernel\": ['linear', 'rbf'],\n",
    "        \"gamma\": ['scale', 'auto']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"HYPERPARAMETER TUNING FOR {best_model_name}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if best_model_name in param_grids:\n",
    "    print(f\"\\nSearching best parameters...\")\n",
    "    print(f\"Parameter grid: {param_grids[best_model_name]}\")\n",
    "    \n",
    "    # Create fresh model instance\n",
    "    models_dict = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "        \"SVM\": SVC(probability=True, random_state=42)\n",
    "    }\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        models_dict[best_model_name], \n",
    "        param_grids[best_model_name], \n",
    "        cv=5, \n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"BEST PARAMETERS FOUND:\")\n",
    "    print(\"-\" * 60)\n",
    "    for param, value in grid.best_params_.items():\n",
    "        print(f\"{param}: {value}\")\n",
    "    \n",
    "    print(f\"\\nBest Cross-Validation Accuracy: {grid.best_score_:.4f}\")\n",
    "    \n",
    "    # Use the best estimator\n",
    "    tuned_model = grid.best_estimator_\n",
    "    best_params = grid.best_params_\n",
    "else:\n",
    "    print(f\"\\nNo hyperparameter grid defined for {best_model_name}\")\n",
    "    tuned_model = best_model\n",
    "    best_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3dad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the tuned model on test set\n",
    "y_pred_tuned = tuned_model.predict(X_test)\n",
    "y_pred_proba_tuned = tuned_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics for tuned model\n",
    "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "precision_tuned = precision_score(y_test, y_pred_tuned, average='binary')\n",
    "recall_tuned = recall_score(y_test, y_pred_tuned, average='binary')\n",
    "f1_tuned = f1_score(y_test, y_pred_tuned, average='binary')\n",
    "roc_auc_tuned = roc_auc_score(y_test, y_pred_proba_tuned)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL MODEL PERFORMANCE (AFTER HYPERPARAMETER TUNING)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Metric':<20} {'Before Tuning':<15} {'After Tuning':<15} {'Improvement':<15}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Accuracy:':<20} {accuracy:<15.4f} {accuracy_tuned:<15.4f} {accuracy_tuned - accuracy:<15.4f}\")\n",
    "print(f\"{'Precision:':<20} {precision:<15.4f} {precision_tuned:<15.4f} {precision_tuned - precision:<15.4f}\")\n",
    "print(f\"{'Recall:':<20} {recall:<15.4f} {recall_tuned:<15.4f} {recall_tuned - recall:<15.4f}\")\n",
    "print(f\"{'F1-Score:':<20} {f1:<15.4f} {f1_tuned:<15.4f} {f1_tuned - f1:<15.4f}\")\n",
    "print(f\"{'ROC-AUC:':<20} {roc_auc:<15.4f} {roc_auc_tuned:<15.4f} {roc_auc_tuned - roc_auc:<15.4f}\")\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748237d8",
   "metadata": {},
   "source": [
    "## 6. Save the Trained Model\n",
    "\n",
    "Save the final optimized model, along with training information and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save the trained model\n",
    "model_path = '../models/women_risk_model.pkl'\n",
    "joblib.dump(tuned_model, model_path)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL SAVED SUCCESSFULLY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nâœ… Model: {best_model_name}\")\n",
    "print(f\"âœ… Saved to: {model_path}\")\n",
    "print(f\"âœ… Test Accuracy: {accuracy_tuned:.4f}\")\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60bf66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model information to text file\n",
    "model_info_path = '../models/model_info.txt'\n",
    "\n",
    "with open(model_info_path, 'w') as f:\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "    f.write(\"WOMEN RISK PREDICTION MODEL INFORMATION\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    f.write(f\"Model: {best_model_name}\\n\")\n",
    "    f.write(f\"Training Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    f.write(\"-\" * 60 + \"\\n\")\n",
    "    f.write(\"Dataset Information:\\n\")\n",
    "    f.write(\"-\" * 60 + \"\\n\")\n",
    "    f.write(f\"Total Samples: {len(data)}\\n\")\n",
    "    f.write(f\"Training Samples: {len(X_train)}\\n\")\n",
    "    f.write(f\"Testing Samples: {len(X_test)}\\n\")\n",
    "    f.write(f\"Number of Features: {X_train.shape[1]}\\n\\n\")\n",
    "    \n",
    "    f.write(\"-\" * 60 + \"\\n\")\n",
    "    f.write(\"Best Hyperparameters:\\n\")\n",
    "    f.write(\"-\" * 60 + \"\\n\")\n",
    "    for param, value in best_params.items():\n",
    "        f.write(f\"{param}: {value}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"-\" * 60 + \"\\n\")\n",
    "    f.write(\"Performance Metrics (Test Set):\\n\")\n",
    "    f.write(\"-\" * 60 + \"\\n\")\n",
    "    f.write(f\"Accuracy:  {accuracy_tuned:.4f}\\n\")\n",
    "    f.write(f\"Precision: {precision_tuned:.4f}\\n\")\n",
    "    f.write(f\"Recall:    {recall_tuned:.4f}\\n\")\n",
    "    f.write(f\"F1-Score:  {f1_tuned:.4f}\\n\")\n",
    "    f.write(f\"ROC-AUC:   {roc_auc_tuned:.4f}\\n\")\n",
    "\n",
    "print(f\"âœ… Model information saved to: {model_info_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d993ce",
   "metadata": {},
   "source": [
    "## 7. Visualize Training History\n",
    "\n",
    "Compare model performance and visualize final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aa5237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance comparison: Before vs After Tuning\n",
    "metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "before_tuning = [accuracy, precision, recall, f1, roc_auc]\n",
    "after_tuning = [accuracy_tuned, precision_tuned, recall_tuned, f1_tuned, roc_auc_tuned]\n",
    "\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars1 = ax.bar(x - width/2, before_tuning, width, label='Before Tuning', color='lightcoral', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, after_tuning, width, label='After Tuning', color='lightgreen', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Metrics', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'Model Performance Comparison - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics_names)\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1.1])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ed5b9f",
   "metadata": {},
   "source": [
    "## âœ… Training Complete!\n",
    "\n",
    "The model has been successfully trained, optimized, and saved. Here's a summary:\n",
    "\n",
    "### ðŸ“Š Model Information\n",
    "- **Best Model:** Check the output above for the selected model\n",
    "- **Model Location:** `../models/women_risk_model.pkl`\n",
    "- **Model Info:** `../models/model_info.txt`\n",
    "\n",
    "### ðŸŽ¯ Key Steps Completed\n",
    "1. âœ… Loaded and prepared processed dataset\n",
    "2. âœ… Split data into training (80%) and testing (20%) sets\n",
    "3. âœ… Trained and compared 5 different ML algorithms\n",
    "4. âœ… Evaluated model performance with multiple metrics\n",
    "5. âœ… Performed hyperparameter tuning using GridSearchCV\n",
    "6. âœ… Saved optimized model for deployment\n",
    "7. âœ… Generated performance visualizations\n",
    "\n",
    "### ðŸ“ˆ Performance Metrics\n",
    "Check the comparison visualization above to see improvements after hyperparameter tuning.\n",
    "\n",
    "### ðŸš€ Next Steps\n",
    "1. **Deploy the Model:** Use the saved model in your Flask application\n",
    "2. **Test Predictions:** Run `test_api.py` to test the API endpoints\n",
    "3. **Monitor Performance:** Track predictions and collect feedback\n",
    "4. **Retrain:** Periodically retrain with new data to improve accuracy\n",
    "\n",
    "---\n",
    "**Note:** Make sure the Flask API (`app.py`) loads the correct model file path."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
